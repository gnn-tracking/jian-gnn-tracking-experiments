{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnn_tracking.preprocessing.point_cloud_builder import PointCloudBuilder\n",
    "import torch_geometric\n",
    "import torch\n",
    "from gnn_tracking.metrics.losses import (\n",
    "    EdgeWeightFocalLoss,\n",
    "    PotentialLoss,\n",
    "    BackgroundLoss,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack data\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "raw_data_dir = \"../raw_data\"\n",
    "output_dir = \"../data\"\n",
    "\n",
    "for filename in os.listdir(raw_data_dir):\n",
    "    if filename.endswith(\".tar.gz\"):\n",
    "        tar_gz_path = os.path.join(raw_data_dir, filename)\n",
    "        tar = tarfile.open(tar_gz_path, \"r:gz\")\n",
    "        tar.extractall(output_dir)\n",
    "        tar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "raw_data = []\n",
    "for filename in os.listdir(output_dir):\n",
    "    directoryname = os.path.join(output_dir, filename)\n",
    "    for data in os.listdir(directoryname):\n",
    "        dataname = os.path.join(directoryname, data)\n",
    "        raw_data.append(torch.load(dataname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n",
      "torch.Size([7285, 7])\n",
      "torch.Size([7285])\n",
      "torch.Size([1112])\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_data))\n",
    "print(raw_data[0].x.size())\n",
    "print(raw_data[0].particle_id.size())\n",
    "print(torch.unique(raw_data[0].particle_id).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravnet = torch_geometric.nn.conv.GravNetConv(in_channels=7, out_channels=3, space_dimensions=8, propagate_dimensions=8, k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_function implementation\n",
    "import math\n",
    "\n",
    "def pairwise_distance(x):\n",
    "    \"\"\"Calculate pairwise distances between vectors (positions) in a batch.\"\"\"\n",
    "    diff = x.unsqueeze(2) - x.t().unsqueeze(0)\n",
    "    dist = torch.sqrt(torch.sum(diff**2, dim=-1) + 1e-8) # add a small number to avoid numerical instability\n",
    "    return dist\n",
    "\n",
    "def gather_max_elements(ID, weight, position):\n",
    "    \"\"\"Find the item with the largest weight for each unique ID.\"\"\"\n",
    "    unique_ids = torch.unique(ID)\n",
    "    max_positions = []\n",
    "    for uid in unique_ids:\n",
    "        indices = (ID == uid)\n",
    "        weights = weight[indices]\n",
    "        positions = position[indices]\n",
    "        max_idx = torch.argmax(weights)\n",
    "        max_positions.append(positions[max_idx])\n",
    "    return torch.stack(max_positions), unique_ids\n",
    "\n",
    "def gather_elements_with_id(ID, uid, position):\n",
    "    \"\"\"Gather elements that share the same ID value.\"\"\"\n",
    "    indices = (ID == uid)\n",
    "    positions = position[indices]\n",
    "    return positions\n",
    "\n",
    "def loss_function(ID, weight, position):\n",
    "    max_positions, unique_ids = gather_max_elements(ID, weight, position)\n",
    "    \n",
    "    mean_distances = []\n",
    "    for i, uid in enumerate(unique_ids):\n",
    "        positions = gather_elements_with_id(ID, uid, position)\n",
    "        diff = positions - max_positions[i]\n",
    "        dist = torch.sum(diff**2, dim=-1) + 1e-8\n",
    "        mean_dist = torch.mean(dist)\n",
    "        mean_distances.append(mean_dist)\n",
    "    A = torch.mean(torch.stack(mean_distances))\n",
    "    \n",
    "    pairwise_dist = pairwise_distance(max_positions)\n",
    "    print(pairwise_dist)\n",
    "    B = torch.mean((1 - pairwise_dist) ** 2)\n",
    "\n",
    "    return 100*A + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.0511,  2.7903],\n",
      "        [ 5.2653,  2.7800],\n",
      "        [ 4.9359,  3.1573],\n",
      "        ...,\n",
      "        [ 7.1659,  3.6903],\n",
      "        [10.0643,  5.5167],\n",
      "        [11.2736,  6.8508]], grad_fn=<SqrtBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(22.6154, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.sigmoid(gravnet(raw_data[2].x)/300)\n",
    "\n",
    "loss_function(raw_data[2].particle_id, y[:, 0], y[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
